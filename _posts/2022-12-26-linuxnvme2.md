---
title:  "Linux NVMe 드라이버 초기화 과정"
excerpt: "Linux NVMe Driver 이야기 #1"

toc: true
toc_sticky: true
published: false
categories:
  - NVMe Storage Technology
tags:
  - Linux NVMe Driver 이야기
---

<br>

> https://testkernelv2.tistory.com/84

# [Linux NVMe 드라이버 초기화 과정](https://mp.weixin.qq.com/s?__biz=MzIwNTUxNDgwNg==&mid=2247484430&idx=1&sn=d6de0ff163106dbe095213b68b121f74&chksm=972ef557a0597c411927a8f1933321ee00cba18d3fcff6f2266322e2eaf004cbd58547027d5a&scene=21#wechat_redirect)

지난 글에서 Linux NVMe 드라이버의 아키텍처와 nvme_core_init() 관련 내용에 대해 알아보았는데, 이번 글에서는 주로 리눅스 NVMe 드라이버의 초기화 과정에서 어떤 일들이 일어나는지 알아봅니다.



pci.c 파일을 열어 **module_init(nvme_init)**을 찾습니다 . 바로 위에 위치한 nvme_init()함수는 코드 구조가 너무 단순합니다. 

```c
static int __init nvme_init(void)
{
	int result;

	//1, 전역 작업 대기열 생성
	nvme_workq = alloc_workqueue("nvme", WQ_UNBOUND | WQ_MEM_RECLAIM, 0);
	if (!nvme_workq)
		return -ENOMEM;

	//2, NVMe 드라이버 등록
	result = pci_register_driver(&nvme_driver); 
	if (result)
		destroy_workqueue(nvme_workq);

	return result;
}

MODULE_AUTHOR("Matthew Wilcox <willy@linux.intel.com>");
MODULE_LICENSE("GPL");
MODULE_VERSION("1.0");
module_init(nvme_init);
module_exit(nvme_exit);
```



## 1단계: 전역 작업 대기열 만들기

Workqueue를 생성할 때 실제로 호출되는 함수는 __alloc_workqueue_key()인데, 이 부분은 이미 Linux의 기본 지식 포인트와 관련되어 있으므로 자세히 공부하지 않고 nvme 관련 내용에 대한 구체적인 분석을 위주로 하겠습니다. 다음은 커널의 두 스레드에 대해 알아보기 위한 __alloc_workqueue_key() 함수의 첫 번째 코드 조합입니다.


创建Workqueue时，实际调用的函数是__alloc_workqueue_key()，这部分已经涉及到Linux底层的知识点，我们不展开详细学习了，还是主要针对nvme相关的内容具体解析。 在这里只是结合__alloc_workqueue_key()函数中最开始的一段代码，学习一下Kernel的两种线程：

```c
	if ((flags & WQ_POWER_EFFICIENT) && wq_power_efficient)
		flags |= WQ_UNBOUND;
```

在kernel中，有两种线程池，**一种是线程池是per cpu的**，也就是说，系统中有多少个cpu，就会创建多少个线程池，cpu x上的线程池创建的worker线程也只会运行在cpu x上。**另外一种是unbound thread pool**，该线程池创建的worker线程可以调度到任意的cpu上去。由于cache locality的原因，per cpu的线程池的性能会好一些，但是对power saving有一些影响。设计往往如此，workqueue需要在performance和power saving之间平衡，想要更好的性能，那么最好让一个cpu上的worker thread来处理work，这样的话，cache命中率会比较高，性能会更好。但是，**从电源管理的角度来看，最好的策略是让idle状态的cpu尽可能的保持idle，而不是反复idle，working，idle again**。

커널에는 두 종류의 스레드 풀이 있습니다. 하나는 스레드 풀이 CPU당이라는 것입니다.Cpu x에서만 실행됩니다. 다른 하나는 바인딩되지 않은 스레드 풀 이며, 이 스레드 풀에서 생성된 작업자 스레드는 모든 CPU에 디스패치될 수 있습니다. 캐시 지역성으로 인해 CPU당 스레드 풀의 성능은 더 좋아지지만 절전에 약간의 영향을 미칩니다. 디자인은 이런 경우가 많습니다.workqueue는 성능과 절전 사이의 균형이 필요합니다.더 나은 성능을 원한다면 CPU의 작업자 스레드가 작업을 처리하도록 하는 것이 가장 좋습니다.이 경우 캐시 적중률이 더 높아질 것입니다. 그리고 성능은 더 좋아질 것입니다. . 그러나 전원 관리의 관점에서 볼 때 가장 좋은 전략은 CPU를 유휴, 작업 및 유휴를 반복하는 대신 가능한 한 유휴 상태로 유지하는 것입니다 .



我们来一个例子辅助理解上面的内容。在t1时刻，work被调度到CPU A上执行，t2时刻work执行完毕，CPU A进入idle，t3时刻有一个新的work需要处理，这时候调度work到那个CPU会好些呢？是处于working状态的CPU B还是处于idle状态的CPU A呢？如果调度到CPU B上运行，那么，由于之前处理过work，其cache内容新鲜，处理起work当然是得心应手，速度很快，但是，这需要将CPU A从idle状态中唤醒。选择CPU B呢就不存在将CPU 从idle状态唤醒，从而获取power saving方面的好处。

위 내용의 이해를 돕기 위해 예를 들어보겠습니다. 시간 t1에 작업이 CPU A에서 실행되도록 예약됨 시간 t2에 작업이 완료되고 CPU A가 유휴 모드로 전환됨 시간 t3에 처리할 새 작업이 있음 이때 CPU는 어떤 CPU입니까? 작업을 예약하는 것이 더 낫습니까? 작동 상태에 있는 CPU B입니까 아니면 유휴 상태에 있는 CPU A입니까? CPU B에서 실행되도록 예약된 경우 작업이 이전에 처리되었기 때문에 캐시 콘텐츠가 신선하고 작업이 편리하고 빠르지만 CPU A가 유휴 상태에서 깨어나야 합니다. CPU B를 선택하면 절전의 이점을 얻기 위해 CPU를 유휴 상태에서 깨울 필요가 없습니다.



了解了上面的基础内容之后，我们再来检视per cpu thread pool和unbound thread pool。当workqueue收到一个要处理的work，如果该workqueue是unbound类型的话，那么该work由unbound thread pool处理并把调度该work去哪一个CPU执行这样的策略交给系统的调度器模块来完成，对于scheduler而言，它会考虑CPU core的idle状态，从而尽可能的让CPU保持在idle状态，从而节省了功耗。因此，**如果一个workqueue有WQ_UNBOUND这样的flag，则说明该workqueue上挂入的work处理是考虑到power saving的**。如果workqueue没有WQ_UNBOUND flag，则说明该workqueue是per cpu的，这时候，调度哪一个CPU core运行worker thread来处理work已经不是scheduler可以控制的了，这样，也就间接影响了功耗。

위의 기본 내용을 이해한 후 CPU별 스레드 풀과 바인딩되지 않은 스레드 풀에 대해 살펴보겠습니다. workqueue가 처리할 작업을 받았을 때 workqueue가 unbound type이면 unbound thread pool에서 작업을 처리하고 어떤 CPU가 실행할 작업을 예약할 것인지에 대한 정책을 시스템 스케줄러 모듈로 넘겨 완료 스케줄러는 CPU 코어의 유휴 상태를 고려하여 CPU를 가능한 한 유휴 상태로 유지하여 전력 소비를 절약합니다. 따라서 workqueue에 WQ_UNBOUND와 같은 플래그가 있으면 workqueue에서 작업을 처리할 때 절전을 고려한다는 의미입니다 . workqueue에 WQ_UNBOUND 플래그가 없으면 workqueue가 cpu당임을 의미하는데, 이때 어느 CPU core가 작업자 쓰레드를 실행하여 작업을 처리할 예정인지는 더 이상 스케줄러의 제어를 받지 못하여 간접적으로 영향을 미친다. 전력 소비.



这块涉及的详细内容，有兴趣的可以翻阅Linux相关书籍。我们接下来进入本文的重点。

이 글과 관련된 자세한 내용은 관심 있는 분들은 리눅스 관련 서적을 읽어보시면 됩니다. 이 기사의 초점으로 넘어 갑시다.

<br>

## 2단계: NVME 드라이버 등록



在初始化过程中，调用kernel提供的pci_register_driver()函数将**nvme_driver**注册到PCI Bus。问题来了，PCI Bus是怎么将nvme driver匹配到对应的NVMe设备的呢？



系统启动时，BIOS会枚举整个PCI Bus, 之后将扫描到的设备通过ACPI tables传给操作系统。当操作系统加载时，PCI Bus驱动则会根据此信息读取各个PCI设备的Header Config空间，从class code寄存器获得一个特征值。class code就是PCI bus用来选择哪个驱动加载设备的唯一根据。NVMe Spec定义NVMe设备的Class code=0x010802h, 如下图。

![img](/assets/images/linuxnvme2-1.png)



根据code来看，nvme driver会将class code写入nvme_id_table,

코드에 따르면 nvme 드라이버는 nvme_id_table에 클래스 코드를 작성합니다.

```c
static struct pci_driver nvme_driver = {

	.name		= "nvme",
	.id_table	= nvme_id_table,
	.probe		= nvme_probe,
	.remove		= nvme_remove,
	.shutdown	= nvme_shutdown,
	.driver		= {
		.pm	= &nvme_dev_pm_ops,
	},
	.sriov_configure = nvme_pci_sriov_configure,
	.err_handler	= &nvme_err_handler,
};
```

nvme_id_table的内容如下，

nvme_id_table의 내용은 다음과 같습니다.

```c
static const struct pci_device_id nvme_id_table[] = {
	{ PCI_VDEVICE(INTEL, 0x0953),
		.driver_data = NVME_QUIRK_STRIPE_SIZE |
				NVME_QUIRK_DISCARD_ZEROES, },

	{ PCI_VDEVICE(INTEL, 0x0a53),
		.driver_data = NVME_QUIRK_STRIPE_SIZE |
				NVME_QUIRK_DISCARD_ZEROES, },

	{ PCI_VDEVICE(INTEL, 0x0a54),
		.driver_data = NVME_QUIRK_STRIPE_SIZE |
				NVME_QUIRK_DISCARD_ZEROES, },

	{ PCI_VDEVICE(INTEL, 0x5845),	/* Qemu emulated controller */
		.driver_data = NVME_QUIRK_IDENTIFY_CNS, },

	{ PCI_DEVICE(0x1c58, 0x0003),	/* HGST adapter */
		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },

	{ PCI_DEVICE(0x1c5f, 0x0540),	/* Memblaze Pblaze4 adapter */
		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },

	{ PCI_DEVICE_CLASS(PCI_CLASS_STORAGE_EXPRESS, 0xffffff) },

	{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2001) },

	{ 0, }
};
```

咦，在上面的nvme_id_table里面怎么没有看到0x010802h呢？原来是最新的linux code将PCI_CLASS_STORAGE_EXPRESS定义放在了pci_ids.h里面了。

위의 nvme_id_table에서 왜 0x010802h가 보이지 않습니까? 최신 Linux 코드는 PCI_CLASS_STORAGE_EXPRESS의 정의를 pci_ids.h에 넣었습니다.

```c
*#define PCI_CLASS_STORAGE_EXPRESS	0x010802*
```

pci_register_driver()函数将**nvme_driver**注册到PCI Bus之后，PCI Bus就明白了这个驱动是给NVMe设备(Class code=0x010802h)用的。

pci_register_driver() 함수 가 **nvme_driver** 를 PCI 버스에 등록한 후 PCI 버스는 이 드라이버가 NVMe 장치용임을 이해합니다(클래스 코드=0x010802h).



到这里，只是找到PCI Bus上面驱动与NVMe设备的对应关系。nvme_init执行完毕，返回后，nvme驱动就啥事不做了，直到pci总线枚举出了这个nvme设备，就开始调用**nvme_probe()**函数开始干活咯。再请出nvme_driver的结构体：

이 시점에서 PCI 버스의 드라이버와 NVMe 장치 간의 대응 관계를 찾으십시오. nvme_init 실행이 완료된 후 반환된 후 nvme 드라이버는 pci 버스가 nvme 장치를 열거한 다음 nvme_probe() 함수를 호출하여 작업을 시작할 때까지 아무 작업도 수행하지 않습니다. 그런 다음 nvme_driver의 구조를 보여주세요.

```c
static struct pci_driver nvme_driver = {
	.name		= "nvme",
	.id_table	= nvme_id_table,
	.probe		= nvme_probe,
	.remove		= nvme_remove,
	.shutdown	= nvme_shutdown,
	.driver		= {
		.pm	= &nvme_dev_pm_ops,
	},
	.sriov_configure = nvme_pci_sriov_configure,
	.err_handler	= &nvme_err_handler,
};
```

想一睹nvme_probe()的风采吗？请听下回分解~

<br>

## 추가 정보

- [Linux의 nvme 드라이버에 대한 자세한 설명](https://developer.aliyun.com/article/596648)
- [Analysis of NVMe Driver Source Code in linux kernel 4.5](https://hyunyoung2.github.io/2016/09/19/NVMe_Driver_Source_Code/?spm=a2c6h.12873639.article-detail.10.3b3ebbebloUoev)
- [NVMe 드라이버 분석 - BAR Space](
