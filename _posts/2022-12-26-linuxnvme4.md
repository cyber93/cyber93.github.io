---
title:  "nvme_reset_work() 함수 분석"
excerpt: "Linux NVMe Driver 이야기 #4"

toc: true
toc_sticky: true
published: false
categories:
  - NVMe Storage Technology
tags:
  - Linux NVMe Driver 이야기
---

<br>

# [nvme_reset_work() 함수 분석](https://mp.weixin.qq.com/s?__biz=MzIwNTUxNDgwNg==&mid=2247484484&idx=1&sn=63d69e8a3ba27cf3d5397daec831e95b&chksm=972ef51da0597c0b6a8d957b0490665605e6b573e4bde444d46f00744f3a8040cfe152cf853b&scene=21#wechat_redirect)

nvme_probe 함수의 마지막 단계에서 nvme_reset_work를 호출하여 재설정 작업을 수행합니다.먼저 nvme_reset_work 함수의 전역 코드를 살펴봅니다.

```c
static void nvme_reset_work(struct work_struct *work)
{
	struct nvme_dev *dev = container_of(work, struct nvme_dev, reset_work);

	int result = -ENODEV;

	// 检查NVME_CTRL_RESETTING标志，来确保nvme_reset_work不会被重复进入.
	if (WARN_ON(dev->ctrl.state == NVME_CTRL_RESETTING))
		goto out;

	// If we're called to reset a live controller first shut it down before moving on.
	if (dev->ctrl.ctrl_config & NVME_CC_ENABLE)
		nvme_dev_disable(dev, false);

	if (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_RESETTING))
		goto out;

	result = nvme_pci_enable(dev);
	if (result)
		goto out;

	result = nvme_configure_admin_queue(dev);
	if (result)
		goto out;

	nvme_init_queue(dev->queues[0], 0);
	result = nvme_alloc_admin_tags(dev);
	if (result)
		goto out;

	result = nvme_init_identify(&dev->ctrl);
	if (result)
		goto out;

	result = nvme_setup_io_queues(dev);
	if (result)
		goto out;

	if (dev->online_queues > 1)
		nvme_queue_async_events(&dev->ctrl);

	mod_timer(&dev->watchdog_timer, round_jiffies(jiffies + HZ));
	// Keep the controller around but remove all namespaces if we don't have any working I/O queue.
	if (dev->online_queues < 2) {
		dev_warn(dev->ctrl.device, "IO queues not created\n");
		nvme_kill_queues(&dev->ctrl);
		nvme_remove_namespaces(&dev->ctrl);
	} else {
		nvme_start_queues(&dev->ctrl);
		nvme_dev_add(dev);
	}

	if (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_LIVE)) {
		dev_warn(dev->ctrl.device, "failed to mark controller live\n");
		goto out;
	}

	if (dev->online_queues > 1)
		nvme_queue_scan(&dev->ctrl);

    return;

 out:
	nvme_remove_dead_ctrl(dev, result);
}
```

위의 코드에 따르면 nvme_reset_work의 주요 작업은 다음과 같이 요약할 수 있음을 알 수 있습니다.

1. nvme_reset_work 함수를 입력한 후 NVME_CTRL_RESETTING 플래그를 먼저 확인하여 nvme_reset_work가 반복적으로 입력되지 않도록 합니다.
2. **nvme_pci_enable** 호출
3. nvme_configure_admin_queue 호출
4. nvme_init_queue 호출
5. nvme_alloc_admin_tags 호출
6. nvme_init_identify 호출
7. nvme_setup_io_queues 호출
8. nvme_start_queues/nvme_dev_add 호출 후 nvme_queue_scan 호출



그런 다음 이러한 단계를 수행하고 단계별로 분석합니다. 먼저 nvme_pci_enable의 내용을 살펴보십시오.

```c
static int nvme_pci_enable(struct nvme_dev *dev)
{
	u64 cap;
	int result = -ENOMEM;
	struct pci_dev *pdev = to_pci_dev(dev->dev);

	// 使能nvme设备的内存空间iomem，也就是之前映射的bar空间。
	if (pci_enable_device_mem(pdev))
		return result;

	// 设置设备具有获得总线的能力，即调用这个函数，使设备具备申请使用PCI总线的能力。
	pci_set_master(pdev);

	// 设定这个nvme设备的DMA区域大小，64 bits或者32 bits
	if (dma_set_mask_and_coherent(dev->dev, DMA_BIT_MASK(64)) &&
	    dma_set_mask_and_coherent(dev->dev, DMA_BIT_MASK(32)))
		goto disable;

	// 读取Controller寄存器NVME_REG_CSTS，判断Controller的状态
	if (readl(dev->bar + NVME_REG_CSTS) == -1) {
		result = -ENODEV;
		goto disable;
	}

	// 为设备分配中断请求，INITx/MSI/MSI-X
	result = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_ALL_TYPES);
	if (result < 0)
		return result;

	// 获取设备64位的Controller Capabilities(CAP)
	cap = lo_hi_readq(dev->bar + NVME_REG_CAP);

	dev->q_depth = min_t(int, NVME_CAP_MQES(cap) + 1, NVME_Q_DEPTH);
	dev->db_stride = 1 << NVME_CAP_STRIDE(cap);

	// 设置Doorbell地址，这里的4096来自SQ Tail DB的起始地址0x1000, 如下图中的Controller寄存器定义。

	dev->dbs = dev->bar + 4096;
	//下面这一段是苹果公司bug，这里忽略
	if (pdev->vendor == PCI_VENDOR_ID_APPLE && pdev->device == 0x2001) {
		dev->q_depth = 2;
		dev_warn(dev->dev, "detected Apple NVMe controller, set "
			"queue depth=%u to work around controller resets\n",
			dev->q_depth);
	}

	// 假如nvme协议的版本大于等于1.2的话，需要调用nvme_map_cmb
	// 映射controller memory buffer. CMB的主要作用是把SQ/CQ存储的位置
	// 从host memory搬到device memory来提升性能，改善延时。
	if (readl(dev->bar + NVME_REG_VS) >= NVME_VS(1, 2, 0)) {
		dev->cmb = nvme_map_cmb(dev);
		if (dev->cmbsz) {
			if (sysfs_add_file_to_group(&dev->ctrl.device->kobj,
						    &dev_attr_cmb.attr, NULL))
				dev_warn(dev->dev,
					 "failed to add sysfs attribute for CMB\n");
		}
	}

	pci_enable_pcie_error_reporting(pdev); // 错误处理
	pci_save_state(pdev); // Suspend之前保存设备当下的状态
	return 0;

 disable:
	pci_disable_device(pdev);
	return result;
}
```

위의 코드에서 우리는 nvme_pci_enable 함수가 실행 중에 많은 nvme 컨트롤러 레지스터를 읽는다는 것을 발견했습니다(아래 표 참조).

![img](/assets/images/linuxnvme4-1.jpg)



nvme_pci_enable 함수에서 수행되는 또 다른 중요한 작업은 장치에 인터럽트를 할당하는 것입니다. nvme 장치는 세 가지 인터럽트 모드를 지원합니다: INITx/MSI/MSI-X 이 세 가지 인터럽트 모드의 구체적인 차이점은 이 공식 계정의 이전 기사 [NVMe 시리즈 주제 5: 인터럽트 메커니즘](http://mp.weixin.qq.com/s?__biz=MzIwNTUxNDgwNg==&mid=2247484377&idx=1&sn=92285dba3a58413415b654b055188640&chksm=972ef280a0597b96dba4a4d1524eeffe08dd04d2020899826cce80d8555fbb2a8bb74dced78b&scene=21#wechat_redirect) 을 참조하십시오 . 인터럽트 모드를 할당하는 pci_alloc_irq_vectors 함수는 실제로 **pci_alloc_irq_vectors_affinity라고 합니다.**

```c
int pci_alloc_irq_vectors_affinity(struct pci_dev *dev, unsigned int min_vecs,
				   unsigned int max_vecs, unsigned int flags,
				   const struct irq_affinity *affd)
{
	static const struct irq_affinity msi_default_affd;
	int vecs = -ENOSPC;

	// 当IRQ为Affinity自动分配时，IRQ中断会分配给所有CPUs 在nvme_pci_enable过程中调用时，*affd=NULL
	if (flags & PCI_IRQ_AFFINITY) {
		if (!affd)
			affd = &msi_default_affd;
        	if (affd->pre_vectors + affd->post_vectors > min_vecs)
			return -EINVAL;
		if (affd->pre_vectors + affd->post_vectors == min_vecs)
			affd = NULL;
	} else {
		if (WARN_ON(affd))
			affd = NULL;
	}

	// 分配MSI-X中断，配置MSI-X capability structure
	if (flags & PCI_IRQ_MSIX) {
		vecs = __pci_enable_msix_range(dev, NULL, min_vecs, max_vecs,
				affd);
		if (vecs > 0)
			return vecs;
	}

	// 分配MSI中断，配置MSI capability structure
	if (flags & PCI_IRQ_MSI) {
		vecs = __pci_enable_msi_range(dev, min_vecs, max_vecs, affd);
		if (vecs > 0)
			return vecs;
	}

	// 分配INITx中断
	if ((flags & PCI_IRQ_LEGACY) && min_vecs == 1) {
		pci_intx(dev, 1);
		return 1;
	}

	return vecs;
}
```

이 세 가지 인터럽트를 동시에 활성화할 수 없다는 점에 유의해야 합니다.예를 들어 MSI-X 인터럽트를 사용하려면 INITx 및 MSI 인터럽트를 비활성화해야 합니다.



지금까지 nvme_pci_enable 함수에 대해 대략적으로 분석해 보았고, nvme_configure_admin_queue 함수에 대해서는 다음 글에서 소개할 예정이니 많은 관심 부탁드립니다!
