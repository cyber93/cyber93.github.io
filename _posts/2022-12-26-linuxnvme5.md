---
title:  "Admin SQ/CQ 생성"
excerpt: "Linux NVMe Driver 이야기 #5"

toc: true
toc_sticky: true

categories:
  - NVMe Storage Technology
tags:
  - Linux NVMe Driver 이야기
---

<br>

# [Admin SQ/CQ 생성](https://mp.weixin.qq.com/s?__biz=MzIwNTUxNDgwNg==&mid=2247484487&idx=1&sn=86faf2642de8f4ebf12e124541315fc2&chksm=972ef51ea0597c085e4e492dea825f76978c9bd9a7442df2964246b78cee23002ac77fb32c69&scene=21#wechat_redirect)

이전 글에서는 nvme_pci_enable() 함수의 내용을 분석해 보았고, 이번에는 nvme_reset_work() 함수의 다른 기능들을 소개하도록 하겠습니다.

> nvme_reset_work() 함수의 주요 작업은 다음과 같이 요약할 수 있습니다.
>
> ⓐ nvme_reset_work() 함수를 입력한 후, NVME_CTRL_RESETTING 플래그를 먼저 확인하여 nvme_reset_work() 함수가 반복적으로 호출되지 않도록 합니다.
>
> ⓑ nvme_pci_enable() 함수 호출
>
> **ⓒ nvme_configure_admin_queue() 함수 호출**
>
> ⓓ nvme_init_queue() 함수 호출
>
> ⓔ nvme_alloc_admin_tags() 함수 호출
>
> ⓕ nvme_init_identify() 함수 호출
>
> ⓖ nvme_setup_io_queues() 함수 호출
>
> ⓗ nvme_start_queues() / nvme_dev_add() 함수 호출 후 nvme_queue_scan() 함수 호출



## ⓒ nvme_configure_admin_queue() 함수

nvme_configure_admin_queue() 함수의 일반적인 단계는 다음과 같다는 것을 알 수 있습니다.

1. CAP 레지스터에서 Subsystem Reset 지원 여부 판단
2. nvme_disable_ctrl() 함수 호출
3. nvme_alloc_queue() 함수 호출
4. nvme_enable_ctrl() 함수 호출
5. queue_request_irq() 함수 호출

```c
static int nvme_configure_admin_queue(struct nvme_dev *dev)
{
	int result;
	u32 aqa;

	//컨트롤러의 CAP 레지스터 내용 가져오기 및 Subsystem Reset 기능 지원 여부 판단
	u64 cap = lo_hi_readq(dev->bar + NVME_REG_CAP);
	struct nvme_queue *nvmeq;
	dev->subsystem = readl(dev->bar + NVME_REG_VS) >= NVME_VS(1, 1, 0) ?
						NVME_CAP_NSSRC(cap) : 0;

	if (dev->subsystem &&
	    (readl(dev->bar + NVME_REG_CSTS) & NVME_CSTS_NSSRO))
		writel(NVME_CSTS_NSSRO, dev->bar + NVME_REG_CSTS);

	result = nvme_disable_ctrl(&dev->ctrl, cap);
	if (result < 0)
		return result;

	nvmeq = dev->queues[0];
	if (!nvmeq) {
		nvmeq = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
		if (!nvmeq)
			return -ENOMEM;
	}

	aqa = nvmeq->q_depth - 1;
	aqa |= aqa << 16;
	writel(aqa, dev->bar + NVME_REG_AQA);
	lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
	lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);

	result = nvme_enable_ctrl(&dev->ctrl, cap);
	if (result)
		return result;

	nvmeq->cq_vector = 0;
	result = queue_request_irq(nvmeq);
	if (result) {
		nvmeq->cq_vector = -1;
		return result;
	}

	return result;
}
```



단계별로 소개하자면 다음과 같습니다.

### 1. NVM Subsystem Reset 지원 여부 판단

먼저 컨트롤러의 CAP 레지스터 비트[36]는 아래 그림과 같이 NVM Subsystem Reset 지원 여부를 정의합니다. 일반적으로 NVM Subsystem은 컨트롤러, NAND 및 NVM Subsystem을 구성하는 인터페이스로 구성된 SSD입니다.
![img](/assets/images/linuxnvme5-1.jpg)



NVMe Sepc.에 정의된 Controller-level Reset에는 5가지 주요 방법이 있습니다 .

- NVM Subsystem Reset
- Conventional Reset (PCI Express Hot, Warm, or Cold reset)
- PCI Express transaction layer Data Link Down status
- Function Level Reset (PCI reset)
- Controller Reset (CC.EN transitions from ‘1’ to ‘0’)

물론 NVMe는 **Controller-level** Reset 정의 외에 비교적 간단한 **Queue-level** Reset도 정의하고 있습니다.



위 Reset에 대한 자세한 내용은 다음을 참조하십시오.

[PCIe 이야기 #17: PCIe 시스템 Reset 모드](/_posts/2022-11-27-pcie17.md)



### 2. nvme_disable_ctrl() 함수

NVMe 컨트롤러를 작동할 때 nvme_disable_ctrl() 함수를 통해 장치를 비활성화한 다음 다시 nvme_enable_ctrl() 함수를 호출하여 장치를 활성화해야 합니다.

```c
int nvme_disable_ctrl(struct nvme_ctrl *ctrl, u64 cap)
{
	int ret;

	ctrl->ctrl_config &= ~NVME_CC_SHN_MASK;
	ctrl->ctrl_config &= ~NVME_CC_ENABLE;

	ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
	if (ret)
		return ret;

	if (ctrl->quirks & NVME_QUIRK_DELAY_BEFORE_CHK_RDY)
		msleep(NVME_QUIRK_DELAY_AMOUNT);

	return nvme_wait_ready(ctrl, cap, false);
}
```

**여기서 ctrl->ops는 이전 nvme_probe() 함수에서 nvme_init_ctrl() 함수 호출 중에 전달된 nvme_pci_ctrl_ops 구조체 입니다.** reg_write32() 함수는 NVME_REG_CC 레지스터를 통해 장치를 비활성화합니다.

이후, nvme_wait__ready() 함수가 상태 레지스터 NVME_REG_CSTS를 읽어 장치가 실제로 정지할 때까지 기다립니다. 타임아웃 상한은 CAP 레지스터의 Bit[31:24]의 Timeout 필드에 따라 계산되며 각 단위는 500ms를 나타냅니다.

![img](/assets/images/linuxnvme5-2.jpeg)

```c
static int nvme_wait_ready(struct nvme_ctrl *ctrl, u64 cap, bool enabled)
{
	unsigned long timeout =
		((NVME_CAP_TIMEOUT(cap) + 1) * HZ / 2) + jiffies;
	u32 csts, bit = enabled ? NVME_CSTS_RDY : 0;
	int ret;

	while ((ret = ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts)) == 0) {
		if (csts == ~0)
			return -ENODEV;
		if ((csts & NVME_CSTS_RDY) == bit)
			break;

		msleep(100);
		if (fatal_signal_pending(current))
			return -EINTR;
		if (time_after(jiffies, timeout)) {
			dev_err(ctrl->device,
				"Device not ready; aborting %s\n", enabled ?
						"initialisation" : "reset");
			return -ENODEV;
		}
	}

	return ret;
}
```



### 3. nvme_alloc_queue() 함수

장치가 비활성화된 후 처음으로 nvmeq를 호출하며 이때 값은 Null입니다. 이때 nvme_alloc_queue() 함수를 호출하여 NVMe 대기열을 할당해야 합니다.

```c
static struct nvme_queue *nvme_alloc_queue(struct nvme_dev *dev, int qid,
							int depth)
{
	struct nvme_queue *nvmeq = kzalloc(sizeof(*nvmeq), GFP_KERNEL);
	if (!nvmeq)
		return NULL;

	nvmeq->cqes = dma_zalloc_coherent(dev->dev, CQ_SIZE(depth),
					  &nvmeq->cq_dma_addr, GFP_KERNEL);
	if (!nvmeq->cqes)
		goto free_nvmeq;

	if (nvme_alloc_sq_cmds(dev, nvmeq, qid, depth))
		goto free_cqdma;

	nvmeq->q_dmadev = dev->dev;
	nvmeq->dev = dev;
	snprintf(nvmeq->irqname, sizeof(nvmeq->irqname), "nvme%dq%d",
			dev->ctrl.instance, qid);
	spin_lock_init(&nvmeq->q_lock);
	nvmeq->cq_head = 0;
	nvmeq->cq_phase = 1;
	nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
	nvmeq->q_depth = depth;
	nvmeq->qid = qid;
	nvmeq->cq_vector = -1;
	dev->queues[qid] = nvmeq;
	dev->queue_count++;

	return nvmeq;

 free_cqdma:
	dma_free_coherent(dev->dev, CQ_SIZE(depth), (void *)nvmeq->cqes,
							nvmeq->cq_dma_addr);
 free_nvmeq:
	kfree(nvmeq);
	return NULL;
}
```

1.먼저 dma_zalloc_coherent를 호출하여 DMA 사용을 위한 완료 대기열용 메모리를 할당합니다. nvmeq->cqes 는 커널에서 사용하기 위해 요청된 메모리의 가상 주소입니다. 그리고 nvmeq->cq_dma_addr 은 DMA 컨트롤러에서 사용하기 위한 이 메모리의 물리적 주소입니다.

2.다음으로 nvme_alloc_sq_cmds를 호출하여 제출 큐를 처리합니다.nvme 버전이 1.2 이상이고 cmb가 제출 큐를 지원하는 경우 cmb를 사용하십시오. 그렇지 않으면 dma_alloc_coherent를 사용하여 완료 대기열과 같은 메모리를 할당합니다.

3.이후 nvmeq->irqname은 등록이 중단되었을 때의 이름을 나타냅니다.nvme%dq%d에서 마지막으로 생성된 nvme0q0 및 nvme0q1, 하나는 admin 대기열용이고 하나는 io 대기열용임을 알 수 있습니다.

nvme_alloc_queue() 함수가 NVMe 대기열을 할당한 후 nvme 관리 대기열의 속성과 할당된 관리 SQ/CQ 메모리 주소를 레지스터에 기록해야 합니다.

```c
writel(aqa, dev->bar +  NVME_REG_AQA );
lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar +  NVME_REG_ASQ );
lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar +  NVME_REG_ACQ );
```

NVMe Spec.은 AQA, ASQ 및 ACQ를 다음과 같이 정의합니다.

![img](/assets/images/linuxnvme5-3.png)

![img](/assets/images/linuxnvme5-4.jpeg)

![img](/assets/images/linuxnvme5-5.jpeg)

4.관리 대기열에 메모리를 할당한 후 nvme_enable_ctrl() 함수를 호출하여 장치를 활성화합니다. 이 함수는 nvme_disable_ctrl() 함수와 유사하나 과정이 역순이므로 여기서는 확장하지 않겠습니다. 2단계에서 nvme_disable_ctrl() 함수 분석을 참고하시면 됩니다.

5.nvme_configure_admin_queue() 함수의 마지막 단계는 queue_request_irq() 함수를 호출하여 중단을 신청하는 것입니다. 이 기능의 주된 역할은 인터럽트 처리 기능을 설정하는 것으로 기본적으로 스레딩의 인터럽트 처리는 사용하지 않고 인터럽트 컨텍스트의 인터럽트 처리를 사용한다.

```c
static int queue_request_irq(struct nvme_queue *nvmeq)
{
	if (use_threaded_interrupts)
		return request_threaded_irq(nvmeq_irq(nvmeq), nvme_irq_check,
				nvme_irq, IRQF_SHARED, nvmeq->irqname, nvmeq);
	else
		return request_irq(nvmeq_irq(nvmeq), nvme_irq, IRQF_SHARED,
				nvmeq->irqname, nvmeq);
}
```



> **Extension** :  **스레딩 및 인터럽트 컨텍스트의 개념** (참고: 이 부분은 Wikipedia에서 가져옴)
>
> 인터럽트 스레딩은 Linux 실시간 성능 을 실현하는 중요한 단계 입니다. 리눅스 표준 커널에서 인터럽트는 최우선 순위 실행 단위입니다.그때 커널이 무엇을 처리하든 인터럽트 이벤트가 있는 한 시스템은 이벤트에 즉시 응답하고 해당 시간에 인터럽트가 해제되지 않는 한 해당 인터럽트 처리 코드를 실행합니다. 따라서 시스템의 네트워크나 I/O 부하가 심할 경우 인터럽트가 매우 빈번하게 발생하고 이후에 발생하는 실시간 작업은 실행될 기회가 거의 없습니다. 즉, 실시간 성능이 전혀 없습니다. **인터럽트가 스레드된 후 인터럽트는 커널 스레드로 실행되고 다른 실시간 우선순위가 부여됩니다. 실시간 태스크는 인터럽트 스레드보다 높은 우선순위를 가질 수 있습니다. 이러한 방식으로 실시간 태스크가 실행될 수 있습니다. 가장 높은 우선 순위 실행 단위로 실시간 보장은 심각한 부하에서도 여전히 사용 가능합니다.**
>
> 
>
> 커널 공간과 사용자 공간은 운영 체제 이론의 기초 중 하나입니다. 즉, 커널 기능 모듈은 커널 공간에서 실행되는 반면 응용 프로그램은 사용자 공간에서 실행됩니다. 최신 CPU는 모두 서로 다른 작동 모드를 가지고 있으며, 이는 서로 다른 수준을 나타내고, 서로 다른 수준은 서로 다른 기능을 가지며, 특정 작업은 하위 수준에서 금지됩니다. 이 하드웨어 기능은 Linux 시스템 설계에 사용되며 최고 수준과 최저 수준의 두 가지 수준을 사용하며 커널은 최상위 수준(커널 모드)에서 실행되며 이 수준은 모든 작업을 수행할 수 있으며 응용 프로그램은 다음 위치에서 실행됩니다. 낮은 수준(사용자 상태), 이 수준에서 프로세서는 하드웨어에 대한 직접 액세스와 메모리에 대한 무단 액세스를 모두 제어합니다. 커널 모드와 사용자 모드에는 자체 메모리 매핑, 즉 자체 주소 공간이 있습니다.
>
> 
>
> 컨텍스트의 개념이 생겨나는 것은 서로 다른 작동 상태의 분할과 함께입니다. 사용자 공간의 응용 프로그램이 물리적 장치를 작동시키거나 장치 공간의 주소를 사용자 공간에 매핑하는 것과 같은 시스템 서비스를 요청하려면 시스템 호출(운영 체제가 사용자 공간에 제공하는 인터페이스 기능)을 통해 구현해야 합니다. 시스템 호출을 통해 사용자 공간의 응용 프로그램은 커널 공간에 들어가고 커널은 프로세스를 대신하여 커널 공간에서 실행되며 컨텍스트 전환이 포함됩니다.사용자 공간과 커널 공간은 다른 주소 매핑, 범용 또는 특수 목적 레지스터 세트 . 사용자 공간의 프로세스는 많은 변수와 매개변수를 커널에 전달해야 하며 커널은 사용자 프로세스의 일부 레지스터와 변수도 저장하므로 시스템 호출이 끝난 후 사용자 공간으로 돌아가 실행을 계속할 수 있습니다. 소위 프로세스 컨텍스트는 프로세스가 실행 중일 때 CPU의 모든 레지스터에 있는 값, 프로세스의 상태 및 스택의 내용 커널이 다른 프로세스로 전환해야 할 때 현재 프로세스의 모든 상태를 저장합니다. 즉, 현재 프로세스의 프로세스 컨텍스트를 저장하여 프로세스가 다시 실행될 때 전환 당시의 상태를 복원하고 실행을 계속할 수 있도록 합니다 .
>
> 
>
> 같은 방식으로 하드웨어는 신호를 트리거하여 커널이 인터럽트 처리기를 호출하고 커널 공간에 들어가도록 합니다. 이 과정에서 하드웨어의 일부 변수와 매개변수도 커널로 전달되는데, 커널은 이러한 매개변수를 통해 인터럽트 처리를 수행하는데, **인터럽트 컨텍스트는 하드웨어가 전달하는 매개변수와 커널이 저장해야 하는 일부 환경으로 이해할 수 있으며, 주로 중단된 프로세스 환경.**
